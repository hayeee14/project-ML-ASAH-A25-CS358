{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Proyek Capstone: AI-Powered Interview Assessment System\n",
        "**Tim A25-CS358**\n",
        "\n",
        "- **Muhammad Rayhan**, M262D5Y1357, sebagai PIC Model & Training (Streamlit/Interface)\n",
        "- **Hafiz Putra Mahesta**, M262D5Y0714, sebagai PIC Integrasi,Model STT, & Fitur (Confidence Score)\n",
        "- **Fahri Rasyidin**, M262D5Y0566, sebagai PIC Data & Evaluasi (Dataset, Kunci Jawaban, WER)"
      ],
      "metadata": {
        "id": "bfb-0BgnKO1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages/Library yang Digunakan"
      ],
      "metadata": {
        "id": "9GiUsL9GLNGF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBArUUN_Tdgq"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install jiwer\n",
        "!pip install moviepy librosa soundfile\n",
        "!pip install streamlit pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import shutil\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import streamlit as st\n",
        "import moviepy.editor as mp\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import whisper\n",
        "import jiwer\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "from pyngrok import ngrok, conf\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "V-Aa70HGjTgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KBUpY7RV5gpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model dan Data"
      ],
      "metadata": {
        "id": "eACBEakarZVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = \"/content/drive/MyDrive/Dataset\"\n",
        "VIDEO_INPUT_DIR = os.path.join(BASE_DIR, \"Video\")\n",
        "AUDIO_OUTPUT_DIR = os.path.join(BASE_DIR, \"Audio\")\n",
        "GROUND_TRUTH_FILE = os.path.join(BASE_DIR, \"Transkrip_Manual\")\n",
        "\n",
        "# Cek Folder Video\n",
        "if os.path.exists(VIDEO_INPUT_DIR):\n",
        "    video_files = [f for f in os.listdir(VIDEO_INPUT_DIR) if f.lower().endswith(('.mp4', '.webm', '.avi', '.mov', '.mkv'))]\n",
        "    print(f\"Folder Video ditemukan.\")\n",
        "    print(f\"Jumlah video yang siap diproses: {len(video_files)} file\")\n",
        "else:\n",
        "    print(f\"Folder Video TIDAK ditemukan di: {VIDEO_INPUT_DIR}\")\n",
        "\n",
        "if os.path.exists(GROUND_TRUTH_FILE):\n",
        "    print(f\"File Transkrip Manual ditemukan.\")\n",
        "else:\n",
        "    print(f\"File Transkrip Manual tidak ditemukan di: {GROUND_TRUTH_FILE}\")\n",
        "\n",
        "try:\n",
        "    model = whisper.load_model(\"small\")\n",
        "    print(\"Model Whisper berhasil dimuat ke dalam sistem.\")\n",
        "except Exception as e:\n",
        "    print(f\"Gagal memuat model: {e}\")"
      ],
      "metadata": {
        "id": "pw5WC7Fwrgpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing Functions"
      ],
      "metadata": {
        "id": "EBBT-U9cdkem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_video_to_audio(video_path, audio_path):\n",
        "    try:\n",
        "        video_clip = mp.VideoFileClip(video_path)\n",
        "        video_clip.audio.write_audiofile(audio_path, codec='pcm_s16le', verbose=False, logger=None)\n",
        "        video_clip.close()\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    try:\n",
        "        technical_prompt = (\n",
        "            \"Transcribe strictly in English. Context: Machine Learning interview. \"\n",
        "            \"Keywords: TensorFlow, Scikit-learn, CNN, Dropout, Overfitting, Transfer Learning. \"\n",
        "            \"Do not include filler words like umm, uh, ah.\"\n",
        "        )\n",
        "\n",
        "        result = model.transcribe(\n",
        "            audio_path,\n",
        "            fp16=False,\n",
        "            language=\"en\",\n",
        "            initial_prompt=technical_prompt\n",
        "        )\n",
        "        return result[\"text\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Transkripsi: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def remove_fillers(text):\n",
        "    #Daftar kata filler yang dihapus\n",
        "    fillers = [\n",
        "        r\"\\bum\\b\", r\"\\buh\\b\", r\"\\buhh\\b\", r\"\\bah\\b\", r\"\\ber\\b\", r\"\\bhmm\\b\",\n",
        "        r\"\\bmhm\\b\", r\"\\buh-huh\\b\", r\"\\bokay\\b\",\n",
        "        r\"\\byou know\\b\", r\"\\bi mean\\b\", r\"\\bkind of\\b\", r\"\\bsort of\\b\",\n",
        "        r\"\\bso\\b\", r\"\\blike\\b\", r\"\\byeah\\b\", r\"\\bright\\b\",\n",
        "    ]\n",
        "\n",
        "    clean_text = text.lower()\n",
        "    for filler in fillers:\n",
        "        clean_text = re.sub(filler, \"\", clean_text)\n",
        "\n",
        "    #Hapus spasi ganda\n",
        "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
        "    return clean_text\n",
        "\n",
        "def calculate_metrics(reference_text, hypothesis_text):\n",
        "    if not reference_text or not hypothesis_text:\n",
        "        return {\"wer\": 1.0, \"accuracy\": 0.0}\n",
        "\n",
        "    #Bersihkan tanda baca dasar\n",
        "    transformation = jiwer.Compose([\n",
        "        jiwer.ToLowerCase(),\n",
        "        jiwer.RemovePunctuation(),\n",
        "        jiwer.RemoveMultipleSpaces(),\n",
        "        jiwer.Strip(),\n",
        "    ])\n",
        "\n",
        "    ref_basic = transformation(reference_text)\n",
        "    hyp_basic = transformation(hypothesis_text)\n",
        "\n",
        "    #Hapus filler words\n",
        "    ref_clean = remove_fillers(ref_basic)\n",
        "    hyp_clean = remove_fillers(hyp_basic)\n",
        "\n",
        "    #Hitung Akurasi\n",
        "    wer_score = jiwer.wer(ref_clean, hyp_clean)\n",
        "    accuracy = max(0, 1 - wer_score) * 100\n",
        "\n",
        "    return {\"wer\": wer_score, \"accuracy\": round(accuracy, 2)}"
      ],
      "metadata": {
        "id": "pqj4uUmUdpW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing Pipeline & Evaluation"
      ],
      "metadata": {
        "id": "efs9kC-we0a5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRANSCRIPT_DIR = os.path.join(BASE_DIR, \"Transkrip_Manual\")\n",
        "\n",
        "#CONVERT VIDEO KE AUDIO\n",
        "video_files = [f for f in os.listdir(VIDEO_INPUT_DIR) if f.lower().endswith(('.mp4', '.avi', '.webm'))]\n",
        "print(f\"\\n[STEP 1] Cek Video: {len(video_files)} file ditemukan.\")\n",
        "\n",
        "for video in tqdm(video_files, desc=\"Converting Videos\"):\n",
        "    v_path = os.path.join(VIDEO_INPUT_DIR, video)\n",
        "    a_path = os.path.join(AUDIO_OUTPUT_DIR, os.path.splitext(video)[0] + \".wav\")\n",
        "\n",
        "    if not os.path.exists(a_path):\n",
        "        convert_video_to_audio(v_path, a_path)\n",
        "\n",
        "#PROSES SEMUA AUDIO\n",
        "all_audio_files = []\n",
        "\n",
        "for f in os.listdir(AUDIO_OUTPUT_DIR):\n",
        "    if f.lower().endswith('.wav'):\n",
        "        all_audio_files.append(os.path.join(AUDIO_OUTPUT_DIR, f))\n",
        "\n",
        "#Hilangkan duplikat path\n",
        "all_audio_files = list(set(all_audio_files))\n",
        "\n",
        "print(f\"\\n[STEP 2] Total File Audio Siap Proses: {len(all_audio_files)} file\")\n",
        "\n",
        "#LOOP TRANSKRIPSI & EVALUASI\n",
        "processing_results = []\n",
        "total_accuracy = 0\n",
        "count_evaluated = 0\n",
        "\n",
        "for audio_path in tqdm(all_audio_files, desc=\"AI Transcribing\"):\n",
        "    filename = os.path.basename(audio_path)\n",
        "    base_name = os.path.splitext(filename)[0]\n",
        "\n",
        "    pred_text = transcribe_audio(audio_path)\n",
        "\n",
        "    metrics = {\"accuracy\": 0.0}\n",
        "    truth_text = \"N/A\"\n",
        "\n",
        "    txt_path = os.path.join(TRANSCRIPT_DIR, base_name + \".txt\")\n",
        "\n",
        "    if os.path.exists(txt_path):\n",
        "        try:\n",
        "            with open(txt_path, 'r', encoding='utf-8') as f:\n",
        "                truth_text = f.read().strip()\n",
        "            #Hitung akurasi\n",
        "            metrics = calculate_metrics(truth_text, pred_text)\n",
        "            total_accuracy += metrics[\"accuracy\"]\n",
        "            count_evaluated += 1\n",
        "        except: pass\n",
        "\n",
        "    #Simpan hasil ke list\n",
        "    processing_results.append({\n",
        "        \"filename\": filename,\n",
        "        \"prediction\": pred_text,\n",
        "        \"ground_truth\": truth_text[:100], # Preview aja\n",
        "        \"accuracy\": metrics[\"accuracy\"]\n",
        "    })\n",
        "\n",
        "print(\"-\" * 50)\n",
        "df_results = pd.DataFrame(processing_results)\n",
        "\n",
        "if count_evaluated > 0:\n",
        "    avg_accuracy = total_accuracy / count_evaluated\n",
        "else:\n",
        "    avg_accuracy = 0.0\n",
        "\n",
        "print(f\"Total Data Diproses       : {len(processing_results)}\")\n",
        "print(f\"Data dengan Kunci Jawaban : {count_evaluated}\")\n",
        "print(f\"Rata-rata Akurasi         : {avg_accuracy:.2f}%\")\n",
        "\n",
        "if avg_accuracy >= 90:\n",
        "    print(\"STATUS: LULUS (Akurasi >= 90%)\")\n",
        "else:\n",
        "    print(\"STATUS: BELUM LULUS\")\n",
        "\n",
        "df_results[[\"filename\", \"accuracy\"]]"
      ],
      "metadata": {
        "id": "QkMpnuiWe2GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Assessment & JSON Generation"
      ],
      "metadata": {
        "id": "xfYF2PONqiO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KEYWORD_DB = {\n",
        "    #Soal Machine Learning\n",
        "    1: [\"challenge\", \"overcame\", \"team\", \"disagreement\", \"listen\", \"meeting\", \"risk\"],\n",
        "    2: [\"transfer learning\", \"vgg\", \"resnet\", \"mobilenet\", \"efficient\", \"keras\", \"accuracy\"],\n",
        "    3: [\"model\", \"accuracy\", \"efficiency\", \"layers\", \"dense\", \"dropout\", \"smote\", \"imbalanced\"],\n",
        "    4: [\"dropout\", \"overfitting\", \"training\", \"layer\", \"rate\", \"neural network\", \"epoch\"],\n",
        "    5: [\"cnn\", \"convolutional\", \"pooling\", \"flatten\", \"filters\", \"image\", \"classification\", \"conv2d\"],\n",
        "    6: [\"background\", \"technology\", \"solve\", \"problems\", \"future\", \"career\", \"engineer\"],\n",
        "    7: [\"python\", \"pandas\", \"scikit-learn\", \"tensorflow\", \"pytorch\", \"preprocessing\", \"tools\"],\n",
        "    8: [\"machine learning\", \"learn\", \"data\", \"patterns\", \"predictions\", \"examples\", \"adapt\"],\n",
        "    9: [\"debug\", \"error\", \"check\", \"data\", \"hyperparameters\", \"learning rate\", \"batch size\"],\n",
        "\n",
        "    #Soal Arsitektur dan General\n",
        "    10: [\"curious\", \"patient\", \"disciplined\", \"problem-solving\", \"adapt\", \"learn\", \"quality\"],\n",
        "    11: [\"creativity\", \"engineering\", \"sustainable\", \"community\", \"design\", \"impact\"],\n",
        "    12: [\"autocad\", \"sketchup\", \"revit\", \"bim\", \"rendering\", \"lumion\", \"3d\", \"software\"],\n",
        "    13: [\"purpose\", \"users\", \"environment\", \"sketches\", \"zoning\", \"flow\", \"light\"],\n",
        "    14: [\"feedback\", \"criticism\", \"collaborative\", \"improve\", \"revise\", \"iteration\", \"open-minded\"],\n",
        "    15: [\"passion\", \"dedication\", \"creativity\", \"analytical\", \"team\", \"value\", \"growth\"],\n",
        "    16: [\"resume\", \"projects\", \"capstone\", \"internship\", \"experience\", \"python\", \"sql\"],\n",
        "    17: [\"challenging\", \"problem\", \"speed\", \"optimization\", \"inference\", \"deploy\", \"solution\"],\n",
        "    18: [\"traveloka\", \"user\", \"product\", \"blog\", \"team\", \"scale\", \"impact\"],\n",
        "    19: [\"teamwork\", \"disagreement\", \"listen\", \"meeting\", \"compromise\", \"result\"],\n",
        "    20: [\"simple\", \"analogy\", \"explain\", \"concept\", \"non-technical\", \"understand\"]\n",
        "}\n",
        "\n",
        "def extract_id_from_filename(filename):\n",
        "    numbers = re.findall(r'\\d+', filename)\n",
        "    if numbers:\n",
        "        return int(numbers[0])\n",
        "    return 999\n",
        "\n",
        "def assess_answer_quality(text, question_id):\n",
        "    \"\"\"Menilai jawaban (0-4)\"\"\"\n",
        "    #Validasi input kosong\n",
        "    if not text or len(text) < 10:\n",
        "        return 0, \"Unanswered\"\n",
        "\n",
        "    target_keywords = KEYWORD_DB.get(question_id, [])\n",
        "    #Fallback jika ID tidak ada\n",
        "    if not target_keywords:\n",
        "        target_keywords = [\"experience\", \"project\", \"learn\", \"team\", \"problem\", \"solution\"]\n",
        "\n",
        "    text_lower = text.lower()\n",
        "    hit_count = sum(1 for k in target_keywords if k in text_lower)\n",
        "    word_count = len(text.split())\n",
        "\n",
        "    score = 2\n",
        "    reason = \"General Response with Limited Details.\"\n",
        "\n",
        "    if word_count > 25:\n",
        "        if hit_count >= 3:\n",
        "            score = 4\n",
        "            reason = \"Comprehensive and Very Clear Response (Contains key technical terms).\"\n",
        "        elif hit_count >= 1:\n",
        "            score = 3\n",
        "            reason = \"Specific Explanation with Basic Understanding.\"\n",
        "\n",
        "    return score, reason\n",
        "\n",
        "def generate_final_report(results_data):\n",
        "    final_output = {\n",
        "        \"success\": True,\n",
        "        \"data\": {\n",
        "            \"id\": 131,\n",
        "            \"candidate\": {\n",
        "                \"name\": \"Hafiz Putra Mahesta\",\n",
        "                \"email\": \"phafiz726@gmail.com\",\n",
        "                \"photoUrl\": \"https://path/to/photo.png\"\n",
        "            },\n",
        "            \"assessorProfile\": {\n",
        "                \"id\": 47,\n",
        "                \"name\": \"AI Assessment System\",\n",
        "                \"photoUrl\": \"https://path/to/system_logo.png\"\n",
        "            },\n",
        "            \"reviewedAt\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"decision\": \"PASSED\",\n",
        "            \"scoresOverview\": {\"project\": 100, \"interview\": 0, \"total\": 0},\n",
        "            \"reviewChecklistResult\": {\n",
        "                \"project\": [],\n",
        "                \"interviews\": {\n",
        "                    \"minScore\": 0, \"maxScore\": 4, \"scores\": []\n",
        "                }\n",
        "            },\n",
        "            \"Overall notes\": \"Automated assessment by AI System based on Whisper transcription.\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    total_interview_score = 0\n",
        "    count_questions = 0\n",
        "\n",
        "    sorted_results = sorted(results_data, key=lambda x: extract_id_from_filename(x['filename']))\n",
        "\n",
        "    for item in sorted_results:\n",
        "        filename = item['filename']\n",
        "        q_id = extract_id_from_filename(filename)\n",
        "\n",
        "        #Penilaian\n",
        "        score, reason = assess_answer_quality(item['prediction'], q_id)\n",
        "\n",
        "        #TRANSKRIP LENGKAP\n",
        "        full_transcript = item['prediction']\n",
        "\n",
        "        checklist_item = {\n",
        "            \"id\": q_id,\n",
        "            \"score\": score,\n",
        "            \"reason\": reason,\n",
        "            \"transcript_preview\": full_transcript\n",
        "        }\n",
        "\n",
        "        final_output[\"data\"][\"reviewChecklistResult\"][\"interviews\"][\"scores\"].append(checklist_item)\n",
        "\n",
        "        #Hitung Total\n",
        "        if 1 <= q_id <= 20:\n",
        "            total_interview_score += score\n",
        "            count_questions += 1\n",
        "\n",
        "    #Finalisasi Skor\n",
        "    if count_questions > 0:\n",
        "        max_possible_score = count_questions * 4\n",
        "        interview_final_score = (total_interview_score / max_possible_score) * 100\n",
        "    else:\n",
        "        interview_final_score = 0\n",
        "\n",
        "    final_output[\"data\"][\"scoresOverview\"][\"interview\"] = round(interview_final_score, 2)\n",
        "\n",
        "    project_score = 100\n",
        "    total_final = (project_score + interview_final_score) / 2\n",
        "    final_output[\"data\"][\"scoresOverview\"][\"total\"] = round(total_final, 2)\n",
        "\n",
        "    final_output[\"data\"][\"decision\"] = \"PASSED\" if total_final >= 75 else \"Need Human Review\"\n",
        "    return final_output\n",
        "\n",
        "if 'processing_results' in locals() and processing_results:\n",
        "    data_to_process = processing_results\n",
        "    print(\"Menggunakan Data Transkripsi ASLI.\")\n",
        "else:\n",
        "    print(\"Data asli tidak ditemukan. Menggunakan Dummy Data untuk Demo.\")\n",
        "    data_to_process = [\n",
        "        {\"filename\": \"interview_question_1.wav\", \"prediction\": \"Dummy answer 1 full text.\" * 10},\n",
        "        {\"filename\": \"interview_question_10.wav\", \"prediction\": \"Dummy answer 10 full text.\" * 10},\n",
        "        {\"filename\": \"interview_question_2.wav\", \"prediction\": \"Dummy answer 2 full text.\" * 10}\n",
        "    ]\n",
        "\n",
        "json_report = generate_final_report(data_to_process)\n",
        "\n",
        "print(json.dumps(json_report, indent=2))\n",
        "\n",
        "output_json_path = os.path.join(BASE_DIR, \"final_assessment_result.json\")\n",
        "with open(output_json_path, \"w\") as f:\n",
        "    json.dump(json_report, f, indent=2)"
      ],
      "metadata": {
        "id": "xfA18IRoqj0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interface Streamlit"
      ],
      "metadata": {
        "id": "mXy-TgFc8Z-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(\"app.py\"):\n",
        "    os.remove(\"app.py\")\n",
        "\n",
        "app_code = \"\"\"import streamlit as st\n",
        "import whisper\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime\n",
        "import moviepy.editor as mp\n",
        "\n",
        "# --- KONFIGURASI HALAMAN ---\n",
        "st.set_page_config(page_title=\"AI Interview Assessor\", layout=\"wide\")\n",
        "\n",
        "# --- 1. LOAD MODEL WHISPER (CACHED) ---\n",
        "# Kita gunakan @st.cache_resource agar model tidak di-load ulang setiap kali klik tombol\n",
        "@st.cache_resource\n",
        "def load_whisper_model():\n",
        "    # Load model 'small' (sesuai yang kita pakai di notebook)\n",
        "    return whisper.load_model(\"small\")\n",
        "\n",
        "try:\n",
        "    model = load_whisper_model()\n",
        "except Exception as e:\n",
        "    st.error(f\"Gagal memuat model Whisper: {e}\")\n",
        "\n",
        "# --- 2. FUNGSI PENDUKUNG (COPY DARI NOTEBOOK) ---\n",
        "# Fungsi Konversi Video -> Audio\n",
        "def convert_video_to_audio(video_path, audio_path):\n",
        "    try:\n",
        "        video_clip = mp.VideoFileClip(video_path)\n",
        "        video_clip.audio.write_audiofile(audio_path, codec='pcm_s16le', verbose=False, logger=None)\n",
        "        video_clip.close()\n",
        "        return True\n",
        "    except: return False\n",
        "\n",
        "# Fungsi Transkripsi\n",
        "def transcribe(audio_path):\n",
        "    technical_prompt = (\n",
        "        \"Transcribe strictly in English. Context: Machine Learning & Architecture interview. \"\n",
        "        \"Keywords: TensorFlow, CNN, Dropout, Overfitting, Scikit-learn, AutoCAD, Revit, BIM. \"\n",
        "        \"Do not include filler words.\"\n",
        "    )\n",
        "    result = model.transcribe(audio_path, fp16=False, language=\"en\", initial_prompt=technical_prompt)\n",
        "    return result[\"text\"].strip()\n",
        "\n",
        "# Fungsi Penilaian (Simple Rule-Based)\n",
        "KEYWORD_DB = {\n",
        "    \"ml\": [\"tensorflow\", \"model\", \"accuracy\", \"layer\", \"training\", \"data\", \"learning\", \"cnn\"],\n",
        "    \"general\": [\"challenge\", \"team\", \"project\", \"solution\", \"role\", \"experience\"]\n",
        "}\n",
        "\n",
        "def assess_answer(text):\n",
        "    if len(text) < 10: return 0, \"Unanswered / Too Short\"\n",
        "\n",
        "    # Hitung kata kunci (Gabungan ML & General)\n",
        "    keywords = KEYWORD_DB[\"ml\"] + KEYWORD_DB[\"general\"]\n",
        "    text_lower = text.lower()\n",
        "    hit_count = sum(1 for k in keywords if k in text_lower)\n",
        "    word_count = len(text.split())\n",
        "\n",
        "    if word_count > 30 and hit_count >= 3:\n",
        "        return 4, \"Comprehensive and Very Clear Response (Contains technical terms).\"\n",
        "    elif word_count > 20 and hit_count >= 1:\n",
        "        return 3, \"Specific Explanation with Basic Understanding.\"\n",
        "    else:\n",
        "        return 2, \"General Response with Limited Details.\"\n",
        "\n",
        "# --- 3. TAMPILAN SIDEBAR (INPUT DATA PENGGUNA) ---\n",
        "with st.sidebar:\n",
        "    st.title(\"Candidate Profile\")\n",
        "    cand_name = st.text_input(\"Full Name\", \"Hafiz Putra Mahesta\")\n",
        "    cand_email = st.text_input(\"Email\", \"hafiz@dicoding.com\")\n",
        "    cand_role = st.selectbox(\"Position Applied\", [\"Machine Learning Engineer\", \"AI Architect\", \"Data Scientist\"])\n",
        "\n",
        "    st.divider()\n",
        "    st.info(\"Upload video wawancara di panel utama untuk memulai analisis.\")\n",
        "\n",
        "# --- 4. TAMPILAN UTAMA ---\n",
        "st.title(\"AI-Powered Interview Assessment\")\n",
        "st.markdown(\"Unggah video wawancara kandidat, dan AI akan melakukan transkripsi serta penilaian otomatis.\")\n",
        "\n",
        "# Input File\n",
        "uploaded_file = st.file_uploader(\"Upload Video Interview (.mp4, .mov, .avi)\", type=[\"mp4\", \"mov\", \"avi\", \"webm\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Tampilkan Video\n",
        "    st.video(uploaded_file)\n",
        "\n",
        "    # Tombol Proses\n",
        "    if st.button(\"Start AI Analysis\", type=\"primary\"):\n",
        "\n",
        "        # A. Simpan File Sementara\n",
        "        os.makedirs(\"temp_upload\", exist_ok=True)\n",
        "        video_path = os.path.join(\"temp_upload\", uploaded_file.name)\n",
        "        audio_path = video_path.replace(\".mp4\", \".wav\").replace(\".webm\", \".wav\")\n",
        "\n",
        "        with open(video_path, \"wb\") as f:\n",
        "            f.write(uploaded_file.getbuffer())\n",
        "\n",
        "        # B. Proses AI (Loading Bar)\n",
        "        with st.status(\"Sedang memproses...\", expanded=True) as status:\n",
        "\n",
        "            st.write(\"Mengonversi Video ke Audio...\")\n",
        "            convert_video_to_audio(video_path, audio_path)\n",
        "\n",
        "            st.write(\"Whisper AI sedang mendengarkan & mentranskrip...\")\n",
        "            transcript_text = transcribe(audio_path)\n",
        "\n",
        "            st.write(\"Melakukan penilaian otomatis...\")\n",
        "            score, reason = assess_answer(transcript_text)\n",
        "\n",
        "            status.update(label=\"Analisis Selesai!\", state=\"complete\", expanded=False)\n",
        "\n",
        "        # C. Tampilkan Hasil\n",
        "        st.divider()\n",
        "        st.subheader(\"Analysis Result\")\n",
        "\n",
        "        c1, c2 = st.columns([2, 1])\n",
        "\n",
        "        with c1:\n",
        "            st.markdown(\"### Transcript\")\n",
        "            st.info(transcript_text)\n",
        "\n",
        "        with c2:\n",
        "            st.markdown(\"### Assessment\")\n",
        "            if score >= 3:\n",
        "                st.success(f\"**Score: {score}/4**\")\n",
        "            else:\n",
        "                st.warning(f\"**Score: {score}/4**\")\n",
        "            st.caption(f\"Reason: {reason}\")\n",
        "\n",
        "        # D. Generate JSON Output (Sesuai Payload)\n",
        "        final_json = {\n",
        "            \"success\": True,\n",
        "            \"data\": {\n",
        "                \"candidate\": {\n",
        "                    \"name\": cand_name,\n",
        "                    \"email\": cand_email,\n",
        "                    \"role\": cand_role\n",
        "                },\n",
        "                \"assessorProfile\": {\n",
        "                    \"id\": 47,\n",
        "                    \"name\": \"AI Assessment System\"\n",
        "                },\n",
        "                \"reviewedAt\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"decision\": \"PASSED\" if score >= 3 else \"REVIEW NEEDED\",\n",
        "                \"scoresOverview\": {\n",
        "                    \"total\": (score/4)*100\n",
        "                },\n",
        "                \"reviewChecklistResult\": {\n",
        "                    \"interviews\": {\n",
        "                        \"scores\": [{\n",
        "                            \"score\": score,\n",
        "                            \"reason\": reason,\n",
        "                            \"transcript_preview\": transcript_text\n",
        "                        }]\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Tombol Download\n",
        "        json_str = json.dumps(final_json, indent=2)\n",
        "        st.download_button(\n",
        "            label=\"Download Official JSON Report\",\n",
        "            data=json_str,\n",
        "            file_name=f\"assessment_{cand_name.replace(' ', '_')}.json\",\n",
        "            mime=\"application/json\"\n",
        "        )\n",
        "\"\"\"\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "print(\"File app.py berhasil diperbarui ke Versi Interaktif!\")"
      ],
      "metadata": {
        "id": "8XBtUfkxJhq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run App"
      ],
      "metadata": {
        "id": "E8rsD83D8qQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Token Ngrok\n",
        "NGROK_AUTH_TOKEN = \"361WrbAMOsmOyORYgVMRd9pI2Q9_3LQJ8vzT92U1wmunBpcJJ\"\n",
        "\n",
        "#Mematikan process lama\n",
        "!pkill -9 streamlit\n",
        "!pkill -9 ngrok\n",
        "\n",
        "#Setup Ngrok Manual\n",
        "if not os.path.exists(\"ngrok\"):\n",
        "    !wget -q -c -nc https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "    !tar -xvzf ngrok-v3-stable-linux-amd64.tgz\n",
        "    !chmod +x ngrok\n",
        "\n",
        "conf.get_default().ngrok_path = \"./ngrok\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "#Menjalankan Streamlit\n",
        "print(\"Menjalankan Streamlit...\")\n",
        "get_ipython().system_raw('streamlit run app.py &')\n",
        "\n",
        "time.sleep(5)\n",
        "try:\n",
        "    public_url = ngrok.connect(8501).public_url\n",
        "    print(f\"\\n KLIK LINK: {public_url}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "aohq4Z6S8xRN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}